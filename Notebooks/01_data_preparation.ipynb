{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892217a9",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3a207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dabc1f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516656b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit records directory: d:\\Vincy-Certificates\\AIDA\\Winter'25\\Thesis\\Prototype\\Streamlit app\\audit_records\n",
      "Output directory: d:\\Vincy-Certificates\\AIDA\\Winter'25\\Thesis\\Prototype\\Notebooks\\data\n",
      "Target workflows: ['new_prompt_analysis', 'dataset_prompt_analysis']\n"
     ]
    }
   ],
   "source": [
    "# Path to audit records directory\n",
    "AUDIT_RECORDS_DIR = Path(r\"d:\\Vincy-Certificates\\AIDA\\Winter'25\\Thesis\\Prototype\\Streamlit app\\audit_records\")\n",
    "OUTPUT_DIR = Path(r\"d:\\Vincy-Certificates\\AIDA\\Winter'25\\Thesis\\Prototype\\Notebooks\\data\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Target workflows to filter\n",
    "TARGET_WORKFLOWS = ['new_prompt_analysis', 'dataset_prompt_analysis']\n",
    "\n",
    "print(f\"Audit records directory: {AUDIT_RECORDS_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Target workflows: {TARGET_WORKFLOWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44be6e5",
   "metadata": {},
   "source": [
    "## Load JSON Files\n",
    "\n",
    "Recursively load all JSON files from the audit_records directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7168829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 JSON files starting with 'audit'\n",
      "Successfully loaded 84 records\n",
      "\n",
      "Total records loaded: 84\n"
     ]
    }
   ],
   "source": [
    "def load_audit_records(audit_dir: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all JSON files from audit_records directory recursively.\n",
    "    Only loads files that start with 'audit'.\n",
    "    \n",
    "    Args:\n",
    "        audit_dir: Path to the audit records directory\n",
    "        \n",
    "    Returns:\n",
    "        List of parsed JSON records\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    json_files = list(audit_dir.rglob('audit*.json'))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files starting with 'audit'\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                # Add file path for reference\n",
    "                data['_source_file'] = str(json_file)\n",
    "                records.append(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing {json_file}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(records)} records\")\n",
    "    return records\n",
    "\n",
    "# Load all records\n",
    "all_records = load_audit_records(AUDIT_RECORDS_DIR)\n",
    "print(f\"\\nTotal records loaded: {len(all_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980643a",
   "metadata": {},
   "source": [
    "## Filter Records\n",
    "\n",
    "Keep only records where workflow is either `new_prompt_analysis` or `dataset_prompt_analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbcbcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered records: 84\n",
      "\n",
      "Workflow distribution:\n",
      "  dataset_prompt_analysis: 42\n",
      "  new_prompt_analysis: 42\n"
     ]
    }
   ],
   "source": [
    "def filter_analysis_workflows(records: List[Dict], target_workflows: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter records to keep only analysis workflows.\n",
    "    \n",
    "    Args:\n",
    "        records: List of all audit records\n",
    "        target_workflows: List of workflow names to keep\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list of records\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    for record in records:\n",
    "        workflow = record.get('workflow', '')\n",
    "        if workflow in target_workflows:\n",
    "            filtered.append(record)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Filter records\n",
    "analysis_records = filter_analysis_workflows(all_records, TARGET_WORKFLOWS)\n",
    "\n",
    "print(f\"Filtered records: {len(analysis_records)}\")\n",
    "print(f\"\\nWorkflow distribution:\")\n",
    "workflow_counts = {}\n",
    "for record in analysis_records:\n",
    "    workflow = record.get('workflow', 'unknown')\n",
    "    workflow_counts[workflow] = workflow_counts.get(workflow, 0) + 1\n",
    "\n",
    "for workflow, count in workflow_counts.items():\n",
    "    print(f\"  {workflow}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d078a26",
   "metadata": {},
   "source": [
    "## Extract Workflow Data\n",
    "\n",
    "Group by workflow_id and extract all required fields into a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b7a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def safe_get(dictionary: Dict, *keys, default=None):\n",
    "    \"\"\"\n",
    "    Safely get nested dictionary values.\n",
    "    \n",
    "    Args:\n",
    "        dictionary: Input dictionary\n",
    "        *keys: Keys to traverse\n",
    "        default: Default value if key doesn't exist\n",
    "        \n",
    "    Returns:\n",
    "        Value at the nested key, or default\n",
    "    \"\"\"\n",
    "    current = dictionary\n",
    "    for key in keys:\n",
    "        if isinstance(current, dict):\n",
    "            current = current.get(key, default)\n",
    "        else:\n",
    "            return default\n",
    "    return current if current is not None else default\n",
    "\n",
    "\n",
    "def extract_cwe_list(vulnerabilities: list) -> list:\n",
    "    \"\"\"\n",
    "    Extract unique CWE IDs from vulnerability list.\n",
    "    \n",
    "    Args:\n",
    "        vulnerabilities: List of vulnerability dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Sorted list of unique CWE IDs\n",
    "    \"\"\"\n",
    "    if not vulnerabilities:\n",
    "        return []\n",
    "    \n",
    "    cwe_ids = set()\n",
    "    for vuln in vulnerabilities:\n",
    "        cwe_id = vuln.get('cwe_id', vuln.get('test_id', ''))\n",
    "        if cwe_id:\n",
    "            # Normalize CWE ID (remove 'CWE-' prefix if present)\n",
    "            cwe_id = str(cwe_id).replace('CWE-', '').strip()\n",
    "            cwe_ids.add(cwe_id)\n",
    "    \n",
    "    return sorted(list(cwe_ids))\n",
    "\n",
    "\n",
    "def count_fix_providers_by_cwe(record: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Count fixes by provider (LLM vs custom rule) and track which unique CWEs were fixed by each.\n",
    "    \n",
    "    Args:\n",
    "        record: Audit record\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with counts for each fix provider and CWE lists\n",
    "    \"\"\"\n",
    "    llm_cwes = set()\n",
    "    rule_cwes = set()\n",
    "    unknown_cwes = set()\n",
    "    \n",
    "    # Check initial patch result\n",
    "    initial_changes = safe_get(record, 'initial_patch_result', 'changes', default=[])\n",
    "    for change in initial_changes:\n",
    "        patch_method = change.get('patch_method', '').lower()\n",
    "        cwe_id = change.get('cwe_id', '')\n",
    "        if cwe_id:\n",
    "            cwe_id = str(cwe_id).replace('CWE-', '').strip()\n",
    "            if 'llm' in patch_method:\n",
    "                llm_cwes.add(cwe_id)\n",
    "            elif 'rule' in patch_method:\n",
    "                rule_cwes.add(cwe_id)\n",
    "            else:\n",
    "                unknown_cwes.add(cwe_id)\n",
    "    \n",
    "    # Check patch iterations\n",
    "    patch_iterations = safe_get(record, 'patch_iterations', default=[])\n",
    "    for iteration in patch_iterations:\n",
    "        changes = iteration.get('changes', [])\n",
    "        for change in changes:\n",
    "            patch_method = change.get('patch_method', '').lower()\n",
    "            cwe_id = change.get('cwe_id', '')\n",
    "            if cwe_id:\n",
    "                cwe_id = str(cwe_id).replace('CWE-', '').strip()\n",
    "                if 'llm' in patch_method:\n",
    "                    llm_cwes.add(cwe_id)\n",
    "                elif 'rule' in patch_method:\n",
    "                    rule_cwes.add(cwe_id)\n",
    "                else:\n",
    "                    unknown_cwes.add(cwe_id)\n",
    "    \n",
    "    return {\n",
    "        'llm': len(llm_cwes),\n",
    "        'rule_based': len(rule_cwes),\n",
    "        'unknown': len(unknown_cwes),\n",
    "        'llm_cwes': sorted(list(llm_cwes)),\n",
    "        'rule_cwes': sorted(list(rule_cwes)),\n",
    "        'unknown_cwes': sorted(list(unknown_cwes))\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_workflow_data(record: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract all required fields from a single workflow record.\n",
    "    \n",
    "    Args:\n",
    "        record: Audit record (can be flat or nested format)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with extracted workflow data\n",
    "    \"\"\"\n",
    "    # Check if this is already a flattened audit record\n",
    "    # (has fields like 'initial_detection_bandit_count' at top level)\n",
    "    is_flat = 'initial_detection_bandit_count' in record\n",
    "    \n",
    "    if is_flat:\n",
    "        # Data is already in the correct format, just return it\n",
    "        workflow = record.get('workflow', '')\n",
    "        return {\n",
    "            'workflow_id': record.get('workflow_id', ''),\n",
    "            'workflow': workflow,\n",
    "            'timestamp': record.get('timestamp', ''),\n",
    "            'file': record.get('file', ''),\n",
    "            'source_file': record.get('source_file', ''),\n",
    "            'prompt_type': 'Manual' if workflow == 'new_prompt_analysis' else 'SecurityEval',\n",
    "            'prompt': record.get('original_content', ''),\n",
    "            'llm_response': record.get('llm_response', ''),\n",
    "            'vulnerabilities_found': record.get('vulnerabilities_found', 0),\n",
    "            'total_vulnerabilities_identified': record.get('total_vulnerabilities_identified', 0),\n",
    "            'total_vulnerabilities_fixed': record.get('total_vulnerabilities_fixed', 0),\n",
    "            'total_vulnerabilities_remaining': record.get('total_vulnerabilities_remaining', 0),\n",
    "            'initial_detection_bandit_count': record.get('initial_detection_bandit_count', 0),\n",
    "            'initial_detection_bandit_cwes': record.get('initial_detection_bandit_cwes', []),\n",
    "            'initial_detection_semgrep_count': record.get('initial_detection_semgrep_count', 0),\n",
    "            'initial_detection_semgrep_cwes': record.get('initial_detection_semgrep_cwes', []),\n",
    "            'initial_detection_ast_count': record.get('initial_detection_ast_count', 0),\n",
    "            'initial_detection_ast_cwes': record.get('initial_detection_ast_cwes', []),\n",
    "            'iteration_detection_bandit_count': record.get('iteration_detection_bandit_count', 0),\n",
    "            'iteration_detection_bandit_cwes': record.get('iteration_detection_bandit_cwes', []),\n",
    "            'iteration_detection_semgrep_count': record.get('iteration_detection_semgrep_count', 0),\n",
    "            'iteration_detection_semgrep_cwes': record.get('iteration_detection_semgrep_cwes', []),\n",
    "            'iteration_detection_ast_count': record.get('iteration_detection_ast_count', 0),\n",
    "            'iteration_detection_ast_cwes': record.get('iteration_detection_ast_cwes', []),\n",
    "            'iterations_count': record.get('iterations_count', 0),\n",
    "            'fixed_cwes': record.get('fixed_cwes', []),\n",
    "            'remaining_cwes': record.get('remaining_cwes', []),\n",
    "            'fix_provider_llm': record.get('fix_provider_llm', 0),\n",
    "            'fix_provider_rule_based': record.get('fix_provider_rule_based', 0),\n",
    "            'fix_provider_unknown': record.get('fix_provider_unknown', 0),\n",
    "            'fix_provider_llm_cwes': record.get('fix_provider_llm_cwes', []),\n",
    "            'fix_provider_rule_cwes': record.get('fix_provider_rule_cwes', []),\n",
    "            'fix_provider_unknown_cwes': record.get('fix_provider_unknown_cwes', []),\n",
    "        }\n",
    "    \n",
    "    # Otherwise, handle nested format (original workflow records)\n",
    "    # Basic information\n",
    "    workflow = record.get('workflow', '')\n",
    "    data = {\n",
    "        'workflow_id': record.get('workflow_id', ''),\n",
    "        'workflow': workflow,\n",
    "        'timestamp': record.get('timestamp', ''),\n",
    "        'file': record.get('file', ''),\n",
    "        'source_file': record.get('_source_file', ''),\n",
    "        'prompt_type': 'Manual' if workflow == 'new_prompt_analysis' else 'SecurityEval',\n",
    "    }\n",
    "    \n",
    "    # Prompt and response\n",
    "    data['prompt'] = record.get('content', record.get('original_content', ''))\n",
    "    data['llm_response'] = record.get('response', '')\n",
    "    \n",
    "    # Vulnerability counts (these include duplicates)\n",
    "    data['vulnerabilities_found'] = record.get('vulnerabilities_found', 0)\n",
    "    data['total_vulnerabilities_identified'] = record.get('total_vulnerabilities_identified', 0)\n",
    "    data['total_vulnerabilities_fixed'] = record.get('total_vulnerabilities_fixed', 0)\n",
    "    data['total_vulnerabilities_remaining'] = record.get('total_vulnerabilities_remaining', 0)\n",
    "    \n",
    "    # Initial detection by tool\n",
    "    initial_run = record.get('initial_run_by_tool', {})\n",
    "    \n",
    "    # Bandit\n",
    "    bandit_vulns = safe_get(initial_run, 'bandit', 'identified_vulnerabilities', default=[])\n",
    "    data['initial_detection_bandit_count'] = safe_get(initial_run, 'bandit', 'count', default=0)\n",
    "    data['initial_detection_bandit_cwes'] = extract_cwe_list(bandit_vulns)\n",
    "    \n",
    "    # Semgrep\n",
    "    semgrep_vulns = safe_get(initial_run, 'semgrep', 'identified_vulnerabilities', default=[])\n",
    "    data['initial_detection_semgrep_count'] = safe_get(initial_run, 'semgrep', 'count', default=0)\n",
    "    data['initial_detection_semgrep_cwes'] = extract_cwe_list(semgrep_vulns)\n",
    "    \n",
    "    # AST (custom_detector)\n",
    "    ast_vulns = safe_get(initial_run, 'custom_detector', 'identified_vulnerabilities', default=[])\n",
    "    data['initial_detection_ast_count'] = safe_get(initial_run, 'custom_detector', 'count', default=0)\n",
    "    data['initial_detection_ast_cwes'] = extract_cwe_list(ast_vulns)\n",
    "    \n",
    "    # Iteration detection by tool\n",
    "    iterations_by_tool = record.get('iterations_by_tool', {})\n",
    "    \n",
    "    # Bandit iterations\n",
    "    bandit_iter_vulns = safe_get(iterations_by_tool, 'bandit', 'identified_vulnerabilities', default=[])\n",
    "    data['iteration_detection_bandit_count'] = safe_get(iterations_by_tool, 'bandit', 'total_across_all_iterations', default=0)\n",
    "    data['iteration_detection_bandit_cwes'] = extract_cwe_list(bandit_iter_vulns)\n",
    "    \n",
    "    # Semgrep iterations\n",
    "    semgrep_iter_vulns = safe_get(iterations_by_tool, 'semgrep', 'identified_vulnerabilities', default=[])\n",
    "    data['iteration_detection_semgrep_count'] = safe_get(iterations_by_tool, 'semgrep', 'total_across_all_iterations', default=0)\n",
    "    data['iteration_detection_semgrep_cwes'] = extract_cwe_list(semgrep_iter_vulns)\n",
    "    \n",
    "    # AST iterations\n",
    "    ast_iter_vulns = safe_get(iterations_by_tool, 'custom_detector', 'identified_vulnerabilities', default=[])\n",
    "    data['iteration_detection_ast_count'] = safe_get(iterations_by_tool, 'custom_detector', 'total_across_all_iterations', default=0)\n",
    "    data['iteration_detection_ast_cwes'] = extract_cwe_list(ast_iter_vulns)\n",
    "    \n",
    "    # Iterations count\n",
    "    data['iterations_count'] = len(record.get('patch_iterations', []))\n",
    "    \n",
    "    # Fixed and remaining CWEs (these are unique)\n",
    "    data['fixed_cwes'] = record.get('fixed_cwe_ids', [])\n",
    "    data['remaining_cwes'] = record.get('non_fixed_cwe_ids', [])\n",
    "    \n",
    "    # Fix provider counts by unique CWE\n",
    "    fix_providers = count_fix_providers_by_cwe(record)\n",
    "    data['fix_provider_llm'] = fix_providers['llm']\n",
    "    data['fix_provider_rule_based'] = fix_providers['rule_based']\n",
    "    data['fix_provider_unknown'] = fix_providers['unknown']\n",
    "    data['fix_provider_llm_cwes'] = fix_providers['llm_cwes']\n",
    "    data['fix_provider_rule_cwes'] = fix_providers['rule_cwes']\n",
    "    data['fix_provider_unknown_cwes'] = fix_providers['unknown_cwes']\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"Data extraction functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2787ea8",
   "metadata": {},
   "source": [
    "## Build DataFrame\n",
    "\n",
    "Process all filtered records and create a DataFrame with one row per workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6fd82f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created with 84 rows and 33 columns\n",
      "\n",
      "DataFrame shape: (84, 33)\n",
      "\n",
      "Column names:\n",
      "  1. workflow_id\n",
      "  2. workflow\n",
      "  3. timestamp\n",
      "  4. file\n",
      "  5. source_file\n",
      "  6. prompt_type\n",
      "  7. prompt\n",
      "  8. llm_response\n",
      "  9. vulnerabilities_found\n",
      "  10. total_vulnerabilities_identified\n",
      "  11. total_vulnerabilities_fixed\n",
      "  12. total_vulnerabilities_remaining\n",
      "  13. initial_detection_bandit_count\n",
      "  14. initial_detection_bandit_cwes\n",
      "  15. initial_detection_semgrep_count\n",
      "  16. initial_detection_semgrep_cwes\n",
      "  17. initial_detection_ast_count\n",
      "  18. initial_detection_ast_cwes\n",
      "  19. iteration_detection_bandit_count\n",
      "  20. iteration_detection_bandit_cwes\n",
      "  21. iteration_detection_semgrep_count\n",
      "  22. iteration_detection_semgrep_cwes\n",
      "  23. iteration_detection_ast_count\n",
      "  24. iteration_detection_ast_cwes\n",
      "  25. iterations_count\n",
      "  26. fixed_cwes\n",
      "  27. remaining_cwes\n",
      "  28. fix_provider_llm\n",
      "  29. fix_provider_rule_based\n",
      "  30. fix_provider_unknown\n",
      "  31. fix_provider_llm_cwes\n",
      "  32. fix_provider_rule_cwes\n",
      "  33. fix_provider_unknown_cwes\n"
     ]
    }
   ],
   "source": [
    "# Extract data from all analysis records\n",
    "workflow_data = []\n",
    "\n",
    "for record in analysis_records:\n",
    "    try:\n",
    "        data = extract_workflow_data(record)\n",
    "        workflow_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing record {record.get('workflow_id', 'unknown')}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(workflow_data)\n",
    "\n",
    "print(f\"DataFrame created with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "086a5678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame cleaned and standardized successfully!\n",
      "\n",
      "Updated shape: (84, 36)\n",
      "\n",
      "New computed columns added:\n",
      "  - unique_cwes_identified: Count of unique CWEs found\n",
      "  - unique_cwes_fixed: Count of unique CWEs fixed\n",
      "  - unique_cwes_remaining: Count of unique CWEs remaining\n"
     ]
    }
   ],
   "source": [
    "def clean_and_standardize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and standardize the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Ensure list columns are actual lists (not strings or NaN)\n",
    "    list_columns = [\n",
    "        'initial_detection_bandit_cwes',\n",
    "        'initial_detection_semgrep_cwes',\n",
    "        'initial_detection_ast_cwes',\n",
    "        'iteration_detection_bandit_cwes',\n",
    "        'iteration_detection_semgrep_cwes',\n",
    "        'iteration_detection_ast_cwes',\n",
    "        'fixed_cwes',\n",
    "        'remaining_cwes',\n",
    "        'fix_provider_llm_cwes',\n",
    "        'fix_provider_rule_cwes',\n",
    "        'fix_provider_unknown_cwes'\n",
    "    ]\n",
    "    \n",
    "    for col in list_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].apply(\n",
    "                lambda x: x if isinstance(x, list) else ([] if pd.isna(x) else [])\n",
    "            )\n",
    "    \n",
    "    # Ensure count columns are integers\n",
    "    count_columns = [\n",
    "        'vulnerabilities_found',\n",
    "        'total_vulnerabilities_identified',\n",
    "        'total_vulnerabilities_fixed',\n",
    "        'total_vulnerabilities_remaining',\n",
    "        'initial_detection_bandit_count',\n",
    "        'initial_detection_semgrep_count',\n",
    "        'initial_detection_ast_count',\n",
    "        'iteration_detection_bandit_count',\n",
    "        'iteration_detection_semgrep_count',\n",
    "        'iteration_detection_ast_count',\n",
    "        'iterations_count',\n",
    "        'fix_provider_llm',\n",
    "        'fix_provider_rule_based',\n",
    "        'fix_provider_unknown'\n",
    "    ]\n",
    "    \n",
    "    for col in count_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # Handle missing string values\n",
    "    string_columns = ['workflow_id', 'workflow', 'timestamp', 'file', 'source_file', 'prompt', 'llm_response', 'prompt_type']\n",
    "    for col in string_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].fillna('')\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'timestamp' in df_clean.columns:\n",
    "        df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Add computed columns for unique CWE counts\n",
    "    if 'fixed_cwes' in df_clean.columns:\n",
    "        df_clean['unique_cwes_fixed'] = df_clean['fixed_cwes'].apply(len)\n",
    "    \n",
    "    if 'remaining_cwes' in df_clean.columns:\n",
    "        df_clean['unique_cwes_remaining'] = df_clean['remaining_cwes'].apply(len)\n",
    "    \n",
    "    # Calculate total unique CWEs identified (fixed + remaining)\n",
    "    if 'fixed_cwes' in df_clean.columns and 'remaining_cwes' in df_clean.columns:\n",
    "        df_clean['unique_cwes_identified'] = df_clean.apply(\n",
    "            lambda row: len(set(row['fixed_cwes']) | set(row['remaining_cwes'])),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning\n",
    "df = clean_and_standardize_dataframe(df)\n",
    "\n",
    "print(\"DataFrame cleaned and standardized successfully!\")\n",
    "print(f\"\\nUpdated shape: {df.shape}\")\n",
    "print(f\"\\nNew computed columns added:\")\n",
    "print(\"  - unique_cwes_identified: Count of unique CWEs found\")\n",
    "print(\"  - unique_cwes_fixed: Count of unique CWEs fixed\")\n",
    "print(\"  - unique_cwes_remaining: Count of unique CWEs remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319e5c1",
   "metadata": {},
   "source": [
    "## Data Cleaning and Standardization\n",
    "\n",
    "Clean and standardize the DataFrame to ensure consistent data types and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "521ad8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete dataset saved to: d:\\Vincy-Certificates\\AIDA\\Winter'25\\Thesis\\Prototype\\Notebooks\\data\\evaluation_base.csv\n",
      "  Total workflows: 84\n"
     ]
    }
   ],
   "source": [
    "# Export complete dataset (used by downstream notebooks)\n",
    "evaluation_base_path = OUTPUT_DIR / 'evaluation_base.csv'\n",
    "df.to_csv(evaluation_base_path, index=False)\n",
    "print(f\"✓ Complete dataset saved to: {evaluation_base_path}\")\n",
    "print(f\"  Total workflows: {len(df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
